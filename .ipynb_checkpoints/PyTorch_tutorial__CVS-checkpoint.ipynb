{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to PyTorch and Neural Networks\n",
    "#### Charles Stewart\n",
    "#### November 26, 2017\n",
    "\n",
    "This is a brief tutorial introduction to PyTorch using a specific example of building\n",
    "a neural network that learns to classify points as inside or outside a hyper sphere\n",
    "of a given radius.\n",
    "\n",
    "We'll start with the problem itself, working purely in NumPy in order to clarify\n",
    "what we are trying to do.  After this, we'll introduce the basic notations of PyTorch, and proceed to two different neural network solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated points in array of dimension (400, 2)\n",
      "Binary labels are in a second array of dimension (400, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKABBEISe8v8B+f3T+cJnniQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10876fc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "def gen_from_sphere( n, center, r_min, r_max):\n",
    "    '''\n",
    "    Generate n points in random directions from a center that are at least\n",
    "    r_min away and at most r_max away.  If r_min is 0 then the points are\n",
    "    within a hyper sphere of radius r_max.  Otherwise, the points are within a\n",
    "    spherical shell whose minimum radius is r_min and whose maximum radius\n",
    "    is r_max.  Note that the points generated in this simple way are not\n",
    "    uniformly distributed in Euclidean space, but that is not a major concern\n",
    "    here.\n",
    "    \n",
    "    The dimensionality of the space is inferred from the dimension of the center\n",
    "    point.  \n",
    "    \n",
    "    Return an array where each row is a data point.  I.e. the array in n X dim\n",
    "    '''\n",
    "    dim = center.size\n",
    "    points = np.broadcast_to(center.reshape(1,dim),(n,dim)) \n",
    "    directions = 2*np.random.random((n,dim))-1\n",
    "    directions /= np.linalg.norm(directions,axis=1).reshape((n,1))\n",
    "    offsets = np.random.random(n) * (r_max-r_min) + r_min\n",
    "    points = points + directions * offsets.reshape((n,1))\n",
    "    return points\n",
    "\n",
    "\n",
    "def gen_points_nested_spheres( n, center, r_min, r_max, debug=False):\n",
    "    '''\n",
    "    Generate n data points in hypersphere, half of which - the \"inliers\" --\n",
    "    are within r_min of the center and half of which -- the \"outliers\" -- are\n",
    "    between r_min and r_max.  Returned are the points in an n X d array\n",
    "    (dim is the dimension of the center point) and a n X 2 binary array\n",
    "    labeling whether the points are inliers (1,0) or outliers (0,1).\n",
    "    \n",
    "    If the debug flag is on, the array shape of the points and the labels\n",
    "    are output.  In addition, if the dimension is 2 plot is generated.\n",
    "    '''\n",
    "    num_inliers = n//2\n",
    "    num_outliers = n - num_inliers\n",
    "    inlier_X = gen_from_sphere( num_inliers, center, 0, r_min)\n",
    "    inlier_Y = np.broadcast_to( np.array([1.,0.]), (num_inliers,2))\n",
    "    outlier_X = gen_from_sphere( num_outliers, center, r_min, r_max)\n",
    "    outlier_Y = np.broadcast_to( np.array([0.,1.]), (num_outliers,2))\n",
    "    X = np.concatenate( (inlier_X,outlier_X), axis=0)\n",
    "    Y = np.concatenate( (inlier_Y,outlier_Y), axis=0)\n",
    "    \n",
    "    if debug:\n",
    "        print('Generated points in array of dimension', X.shape)\n",
    "        print('Binary labels are in a second array of dimension', Y.shape)\n",
    "        \n",
    "        if center.size == 2:\n",
    "            from matplotlib import pyplot as plt\n",
    "            plt.scatter( inlier_X[:,0], inlier_X[:,1], marker='+' )\n",
    "            plt.scatter( outlier_X[:,0], outlier_X[:,1], marker='o' )\n",
    "            plt.show()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# Generate an example in 2d, just to see the point distribution\n",
    "# The \"real\" data \n",
    "num_points = 400\n",
    "center = np.array( [1.5, 6.4])\n",
    "r_min, r_max = 5, 10\n",
    "X,Y = gen_points_nested_spheres( num_points, center, r_min, r_max, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "Now for PyTorch.  The starting point is http://pytorch.org/tutorials/ for tutorials and http://pytorch.org/docs/master/ for extensive documentation.  On the tutorials site, I found both the \"60 Minute Blitz\" and \"Learning PyTorch with Examples\" to be useful, but preferred the latter.  It starts with a simple neural network implemented in NumPy, then converts this to use of pytorch Tensors.  Next it introduces Variables and their most important property, automatic gradient calculation (\"autograd\").  At that point it has two subsections, one on defining new autograd functions and the second comparing to TensorFlow Static Graphs.  Neither of these is terribly important on first reading.  Instead, I'd suggest skipping ahead to the nn module.  I suggest going back to the 60 Minute Blitz for example of convolutional layes\n",
    "\n",
    "We'll start here with a brief discussion of Tensors and Variables and then jump right into Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tensor: w\n",
      "\n",
      "-0.8235  0.6950  0.6873\n",
      " 1.6673  0.1652  2.8778\n",
      "-1.3196  0.8969 -2.6443\n",
      " 0.7130 -0.6525 -1.3374\n",
      "-2.2134  1.4594  0.4219\n",
      " 0.1745  0.1956 -1.8177\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# The first important property of torch is the notion of a Tensor.  This is essentially\n",
    "# the same as a NumPy array, but it has CUDA / GPU capabilities when a GPU is available.\n",
    "# We can easily convert back and forth between Tensors and arrays without deep copying\n",
    "\n",
    "# The following creates a 2d tensor with 6 rows and 3 columns, filled in with random\n",
    "# values from a normal distribution of mean 0 and variance 1.\n",
    "w = torch.randn(6, 3)\n",
    "print('First tensor: w')\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced verson of W is x:\n",
      "\n",
      "-1.3196  0.8969\n",
      " 0.7130 -0.6525\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "We changed x[0,1] to 20.0 and now w[2,1] is also 20.0\n"
     ]
    }
   ],
   "source": [
    "# Tensors can be sliced and this works without deep copying\n",
    "x = w[2:4, 0:2]\n",
    "print('Sliced verson of W is x:')\n",
    "print(x)\n",
    "x[0,1] = 20\n",
    "print('We changed x[0,1] to', x[0,1], 'and now w[2,1] is also', w[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-component vector as a tensor: \n",
      " 0.0846\n",
      " 0.6285\n",
      " 0.9018\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Component-wise multiplication, with broadcasting\n",
      "\n",
      " -0.0697   0.4368   0.6198\n",
      "  0.1411   0.1038   2.5952\n",
      " -0.1117  12.5690  -2.3846\n",
      "  0.0603  -0.4101  -1.2060\n",
      " -0.1873   0.9172   0.3805\n",
      "  0.0148   0.1229  -1.6392\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Create a row vector and then do component-wise multiplication\n",
    "u = torch.randn( 3 )\n",
    "print('3-component vector as a tensor:', u)\n",
    "print('Component-wise multiplication, with broadcasting')\n",
    "print(w*u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -0.5908\n",
      " -3.0122\n",
      " 12.2704\n",
      "  1.3478\n",
      " -0.1281\n",
      "  2.2238\n",
      "[torch.FloatTensor of size 6x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Now create a 3x1 tensor--- essentially a column vector --- and do\n",
    "#  matrix multiplication\n",
    "u = torch.randn( 3,1 )\n",
    "print( torch.mm(w,u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is the numpy version of u:\n",
      " [[ 5.        ]\n",
      " [ 0.47000492]\n",
      " [-1.1587795 ]]\n",
      "b and c are back to pytorch: \n",
      " 5.0000\n",
      " 0.4700\n",
      "-1.1588\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "We changed a[0,0] and this changes b and c as well:\n",
      "b[0,0] = 5.0 and c[0,0] = 5.0\n"
     ]
    }
   ],
   "source": [
    "#  Many of the methods you'd expect for NumPy arrays exist for PyTorch tensors.  See\n",
    "#  extensive documentation at http://pytorch.org/docs/master/torch.html#tensors\n",
    "\n",
    "# We can go back and forth between NumPy arrays and PyTorch tensors\n",
    "# through shallow copies\n",
    "a = u.numpy()\n",
    "print('a is the numpy version of u:\\n', a)\n",
    "b = torch.from_numpy(a)\n",
    "c = torch.Tensor(b)\n",
    "print('b and c are back to pytorch:', b)\n",
    "a[0,0] = 5\n",
    "print('We changed a[0,0] and this changes b and c as well:')\n",
    "print('b[0,0] =', b[0,0], 'and c[0,0] =', c[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor c is \n",
      " -4.5876\n",
      "  5.0796\n",
      "  5.8662\n",
      "  4.8080\n",
      "-10.8697\n",
      "  3.0705\n",
      "[torch.FloatTensor of size 6x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So why create a separate class for tensors when they seem to have\n",
    "# the same attributes as as NumPy arrays?  There are two answers.\n",
    "\n",
    "# First, tensors are designed to work with CUDA when a GPU is available,\n",
    "# but this is transparent to the rest of the code:\n",
    "if torch.cuda.is_available():\n",
    "    w = w.cuda()\n",
    "    b = b.cuda()\n",
    "\n",
    "c = torch.mm(w,b)\n",
    "print('Tensor c is', c)  # Proceeds whether on CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Variables\n",
    "The second reason for PyTorch tensors is that they are designed to work with our second\n",
    "major PyTorch construct:  Variables.  A Variable has three major attributes:\n",
    "1. data --- A PyTorch tensor.   \n",
    "2. grad --- The gradient tensor of the variable with respect to the final cost function it is used in\n",
    "3. creator - The link to the function the Variable is created from.  More on this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminding ourselves:  w= \n",
      " -0.8235   0.6950   0.6873\n",
      "  1.6673   0.1652   2.8778\n",
      " -1.3196  20.0000  -2.6443\n",
      "  0.7130  -0.6525  -1.3374\n",
      " -2.2134   1.4594   0.4219\n",
      "  0.1745   0.1956  -1.8177\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Reminding ourselves:  w=', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv= Variable containing:\n",
      " -0.8235   0.6950   0.6873\n",
      "  1.6673   0.1652   2.8778\n",
      " -1.3196  20.0000  -2.6443\n",
      "  0.7130  -0.6525  -1.3374\n",
      " -2.2134   1.4594   0.4219\n",
      "  0.1745   0.1956  -1.8177\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n",
      "wv.data = \n",
      " -0.8235   0.6950   0.6873\n",
      "  1.6673   0.1652   2.8778\n",
      " -1.3196  20.0000  -2.6443\n",
      "  0.7130  -0.6525  -1.3374\n",
      " -2.2134   1.4594   0.4219\n",
      "  0.1745   0.1956  -1.8177\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n",
      "Slicing works on variables: Variable containing:\n",
      " 20.0000  -2.6443\n",
      " -0.6525  -1.3374\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now create a variable from w, telling the constructor that we want\n",
    "# to be able to compute gradients on this variable.\n",
    "wv = Variable(w, requires_grad=True)\n",
    "print(\"wv=\", wv)\n",
    "print('wv.data =', wv.data)\n",
    "print('Slicing works on variables:', wv[2:4,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is Variable containing:\n",
      " 24.1533\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Next create a simple cost function\n",
    "cost = torch.mean( wv*wv )  # Cost is the square magnitude of the elments of the matrix\n",
    "print('cost is', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now comes the gradient!\n",
    "By calling the method *backward*, we backpropagate the gradient through the computation used to create the cost function value.  In this simple case, we are computing the gradient with respect to the terms of our our small $m \\times n$ ($ 6 \\times 3$) 2d tensor.  The result should be another 2d tensor whose values are $\\frac{2}{m n} w[i,j]$. When we ask for the values of the gradient we get exactly what we predicted.  For example, the 2,2 entry in the gradient, corresponding to the value 20 in wv, is $2/18\\times 20$ as the following shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv.grad= Variable containing:\n",
      "-0.0915  0.0772  0.0764\n",
      " 0.1853  0.0184  0.3198\n",
      "-0.1466  2.2222 -0.2938\n",
      " 0.0792 -0.0725 -0.1486\n",
      "-0.2459  0.1622  0.0469\n",
      " 0.0194  0.0217 -0.2020\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost.backward()\n",
    "print('wv.grad=', wv.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creator / grad_fn\n",
    "\n",
    "In order to briefly discuss the creator function, we take this further in the following example by combining several variables, similar to the computation in a fully connected layer of a neural network.  This combination can be thought of as a simple graph where the root is the cost function, each node in the middle is the result of an intermediate function, and our leaves are the initial variables we are interested in. The nodes have grad_fn that point back to the previous nodes (or leaves) combined to create them.  Leaves have no grad_fn.\n",
    "\n",
    "The print statements from the following only show these grad_fn values, but if you wish you can add print statements to calculate the gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost.grad_fn: <torch.autograd.function.MeanBackward object at 0x10fa2e6d8>\n",
      "mult.grad_fn: <torch.autograd.function.AddBackward object at 0x10fa2e5e8>\n",
      "av.grad_fn: None\n",
      "bv.grad_fn: None\n",
      "wv.grad_fn: None\n",
      "av.grad: Variable containing:\n",
      "-0.3003\n",
      " 3.6438\n",
      "-0.3021\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "mult.grad: None\n"
     ]
    }
   ],
   "source": [
    "av = Variable(torch.randn( 3, 1),requires_grad = True)\n",
    "bv = Variable(torch.randn( 6, 1),requires_grad = True )\n",
    "mult = torch.mm(wv,av) + bv\n",
    "cost = torch.mean(mult)\n",
    "cost.backward()\n",
    "print('cost.grad_fn:', cost.grad_fn)\n",
    "print('mult.grad_fn:', mult.grad_fn)\n",
    "print('av.grad_fn:', av.grad_fn)\n",
    "print('bv.grad_fn:', bv.grad_fn)\n",
    "print('wv.grad_fn:', wv.grad_fn)\n",
    "print('av.grad:', av.grad)\n",
    "print('mult.grad:', mult.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks in PyTorch\n",
    "Using Variables and the build up of functions between them, we can create neural network by hand.  Doing so is a good exercise in understanding the basics of PyTorch and if you are going to do significant work in neural networks I recommend that you do so.  For our purposes, however, we are going to switch gears to the higher-level capabilities in PyTorch for defining an using neural networks from standard components.\n",
    "\n",
    "Before getting started, we return to our hypersphere classification problem, and generate the data we will actually use to train, validata and test our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated points in array of dimension (640, 4)\n",
      "Binary labels are in a second array of dimension (640, 2)\n",
      "generate_and_convert for Train  X size  torch.Size([640, 4]) , Y size torch.Size([640, 2])\n",
      "Generated points in array of dimension (128, 4)\n",
      "Binary labels are in a second array of dimension (128, 2)\n",
      "generate_and_convert for Valid  X size  torch.Size([128, 4]) , Y size torch.Size([128, 2])\n",
      "Generated points in array of dimension (128, 4)\n",
      "Binary labels are in a second array of dimension (128, 2)\n",
      "generate_and_convert for Valid  X size  torch.Size([128, 4]) , Y size torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "#  The points, the center location and the dimensions of the \n",
    "#  internal hypersphere and the external disk.\n",
    "point_dim = 4\n",
    "center = 10*np.random.randn(point_dim)\n",
    "r_min, r_max = 4, 8\n",
    "\n",
    "def generate_and_convert( n, center, r_min, r_max, debug=True, name=''):\n",
    "    '''\n",
    "    Generate the data for the interior and exterior points and convert\n",
    "    to PyTorch variables.\n",
    "    '''\n",
    "    X, Y = gen_points_nested_spheres( n, center, r_min, r_max, debug)\n",
    "    X, Y = Variable(torch.Tensor(X)), Variable(torch.Tensor(Y))\n",
    "    if debug:\n",
    "        print('generate_and_convert for', name, ' X size ', X.size(), ', Y size', Y.size())\n",
    "    return X, Y\n",
    "\n",
    "n_train, n_valid, n_test = 640, 128, 128\n",
    "X_train, Y_train = generate_and_convert( n_train, center, r_min, r_max, debug=True, name='Train')\n",
    "X_valid, Y_valid = generate_and_convert( n_valid, center, r_min, r_max, debug=True, name='Valid')\n",
    "X_test, Y_test = generate_and_convert( n_test, center, r_min, r_max, debug=True, name='Valid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a simple fully connected network\n",
    "\n",
    "The cell below shows the definition of a neural network with two hidden layers of 15 nodes each.\n",
    "It is defined as a subclass of the nn.Module class, so the first step in the initializer is to call the initializer for the base class.\n",
    "The initializer for our new class creates the layers --- each fully-connected --- but does not hook them together.  This is done in the forward function.\n",
    "You will also notice that there is no definition of the backward function, even though it is used.\n",
    "This function is created in the base class based on the result of the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (fc1): Linear (4 -> 15)\n",
      "  (fc2): Linear (15 -> 15)\n",
      "  (fc3): Linear (15 -> 2)\n",
      ")\n",
      "torch.Size([15, 4])\n",
      "torch.Size([15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15])\n",
      "torch.Size([2, 15])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "N0 = point_dim\n",
    "N1 = 15\n",
    "N2 = 15\n",
    "Nout = 2  \n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Create three fully connected layers, two of which are hidden and the third is\n",
    "        # the output layer.  In each case, the first argument o Linear is the number\n",
    "        # of input values from the previous layer, and the second argument is the number\n",
    "        # of nodes in this layer.  The call to the Linear initializer creates a PyTorch\n",
    "        # functional that in turn adds a weight matrix and a bias vector to the list of\n",
    "        # (learnable) parameters stored with each Net object.  These weight matrices\n",
    "        # and bias vectors are implicitly initialized using a normal distribution\n",
    "        # with mean 0 and variance 1\n",
    "        self.fc1 = nn.Linear( N0, N1, bias=True)\n",
    "        self.fc2 = nn.Linear( N1, N2, bias=True)\n",
    "        self.fc3 = nn.Linear( N2, Nout, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  The forward method takes an input Variable and creates a chain of Variables\n",
    "        #  from the layers of the network defined in the initializer. The F.relu is\n",
    "        #  a functional implementing the Rectified Linear activation function.\n",
    "        #  Notice that the output layer does not include the activation function.\n",
    "        #  As we will see, that is combined into the criterion for the loss function.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "#  Create an instance of this network.\n",
    "net = Net()\n",
    "\n",
    "#  Define the Mean Squared error loss function as the criterion for this network's training\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#  Print a summary of the network.  Notice that this only shows the layers\n",
    "print(net)\n",
    "\n",
    "#  Print a s\n",
    "params = list(net.parameters())\n",
    "print(params[0].size()) # The parameter holding the layer 1 weight matrix\n",
    "print(params[1].size()) # ... the layer 1 bias vector\n",
    "print(params[2].size()) # ... the layer 2 weight matrix\n",
    "print(params[3].size()) # ... the layer 3 bias vector\n",
    "print(params[4].size()) # ... the layer 4 weight vector\n",
    "print(params[5].size()) # ... the layer 5 bias vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Training the network\n",
    "\n",
    "Now we can write our own training function using a form of stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.24533\n",
      "Epoch 10 loss: 0.24400\n",
      "Epoch 20 loss: 0.24278\n",
      "Epoch 30 loss: 0.24159\n",
      "Epoch 40 loss: 0.24042\n",
      "Epoch 50 loss: 0.23925\n",
      "Epoch 60 loss: 0.23812\n",
      "Epoch 70 loss: 0.23698\n",
      "Epoch 80 loss: 0.23583\n",
      "Epoch 90 loss: 0.23467\n",
      "Epoch 100 loss: 0.23352\n",
      "Epoch 110 loss: 0.23238\n",
      "Epoch 120 loss: 0.23125\n",
      "Epoch 130 loss: 0.23015\n",
      "Epoch 140 loss: 0.22905\n",
      "Epoch 150 loss: 0.22801\n",
      "Epoch 160 loss: 0.22699\n",
      "Epoch 170 loss: 0.22599\n",
      "Epoch 180 loss: 0.22503\n",
      "Epoch 190 loss: 0.22407\n",
      "Epoch 200 loss: 0.22312\n",
      "Epoch 210 loss: 0.22220\n",
      "Epoch 220 loss: 0.22127\n",
      "Epoch 230 loss: 0.22036\n",
      "Epoch 240 loss: 0.21946\n",
      "Epoch 250 loss: 0.21856\n",
      "Epoch 260 loss: 0.21768\n",
      "Epoch 270 loss: 0.21679\n",
      "Epoch 280 loss: 0.21591\n",
      "Epoch 290 loss: 0.21503\n",
      "Epoch 300 loss: 0.21415\n",
      "Epoch 310 loss: 0.21327\n",
      "Epoch 320 loss: 0.21239\n",
      "Epoch 330 loss: 0.21152\n",
      "Epoch 340 loss: 0.21065\n",
      "Epoch 350 loss: 0.20979\n",
      "Epoch 360 loss: 0.20891\n",
      "Epoch 370 loss: 0.20804\n",
      "Epoch 380 loss: 0.20717\n",
      "Epoch 390 loss: 0.20631\n",
      "Epoch 400 loss: 0.20542\n",
      "Epoch 410 loss: 0.20454\n",
      "Epoch 420 loss: 0.20365\n",
      "Epoch 430 loss: 0.20276\n",
      "Epoch 440 loss: 0.20186\n",
      "Epoch 450 loss: 0.20097\n",
      "Epoch 460 loss: 0.20007\n",
      "Epoch 470 loss: 0.19918\n",
      "Epoch 480 loss: 0.19827\n",
      "Epoch 490 loss: 0.19737\n",
      "Epoch 500 loss: 0.19645\n",
      "Epoch 510 loss: 0.19555\n",
      "Epoch 520 loss: 0.19460\n",
      "Epoch 530 loss: 0.19372\n",
      "Epoch 540 loss: 0.19273\n",
      "Epoch 550 loss: 0.19176\n",
      "Epoch 560 loss: 0.19078\n",
      "Epoch 570 loss: 0.18986\n",
      "Epoch 580 loss: 0.18887\n",
      "Epoch 590 loss: 0.18788\n",
      "Epoch 600 loss: 0.18696\n",
      "Epoch 610 loss: 0.18598\n",
      "Epoch 620 loss: 0.18500\n",
      "Epoch 630 loss: 0.18402\n",
      "Epoch 640 loss: 0.18309\n",
      "Epoch 650 loss: 0.18218\n",
      "Epoch 660 loss: 0.18129\n",
      "Epoch 670 loss: 0.18039\n",
      "Epoch 680 loss: 0.17951\n",
      "Epoch 690 loss: 0.17858\n",
      "Epoch 700 loss: 0.17769\n",
      "Epoch 710 loss: 0.17681\n",
      "Epoch 720 loss: 0.17592\n",
      "Epoch 730 loss: 0.17509\n",
      "Epoch 740 loss: 0.17422\n",
      "Epoch 750 loss: 0.17338\n",
      "Epoch 760 loss: 0.17251\n",
      "Epoch 770 loss: 0.17167\n",
      "Epoch 780 loss: 0.17081\n",
      "Epoch 790 loss: 0.16997\n",
      "Epoch 800 loss: 0.16916\n",
      "Epoch 810 loss: 0.16837\n",
      "Epoch 820 loss: 0.16758\n",
      "Epoch 830 loss: 0.16679\n",
      "Epoch 840 loss: 0.16599\n",
      "Epoch 850 loss: 0.16525\n",
      "Epoch 860 loss: 0.16450\n",
      "Epoch 870 loss: 0.16374\n",
      "Epoch 880 loss: 0.16297\n",
      "Epoch 890 loss: 0.16225\n",
      "Epoch 900 loss: 0.16150\n",
      "Epoch 910 loss: 0.16079\n",
      "Epoch 920 loss: 0.16009\n",
      "Epoch 930 loss: 0.15941\n",
      "Epoch 940 loss: 0.15872\n",
      "Epoch 950 loss: 0.15810\n",
      "Epoch 960 loss: 0.15737\n",
      "Epoch 970 loss: 0.15680\n",
      "Epoch 980 loss: 0.15612\n",
      "Epoch 990 loss: 0.15550\n",
      "Final training loss is 0.15835\n",
      "Final test loss: 0.15327\n"
     ]
    }
   ],
   "source": [
    "#  Set parameters to control the process\n",
    "epochs = 1000\n",
    "batch_size = 16\n",
    "n_batches = int(np.ceil(n_train / batch_size))\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#  Compute an initial loss using all of the validation data.\n",
    "#\n",
    "#  A couple of notes are important here:\n",
    "#  (1) X_valid contains all of the validation input, with each validation\n",
    "#      data instance being a row of X_valid\n",
    "#  (2) Therefore, pred_Y_valid is a Variable containing the output layer\n",
    "#      activations for each of the validation inputs.\n",
    "#  (3) This is accomplished through the function call net(X_valid), which in\n",
    "#      turn calls the forward method under the hood to figure out the flow of\n",
    "#      the data and activations in the network.\n",
    "pred_Y_valid = net(X_valid)\n",
    "valid_loss = criterion(pred_Y_valid, Y_valid)\n",
    "print(\"Initial loss: %.5f\" %valid_loss.data[0])\n",
    "\n",
    "for ep in range(epochs):\n",
    "    #  Create a random permutation of the indices of the row vectors.\n",
    "    indices = torch.randperm(n_train)\n",
    "    \n",
    "    #  Run through each mini-batch\n",
    "    for b in range(n_batches):\n",
    "        #  Use slicing (of the pytorch Variable) to extract the\n",
    "        #  indices and then the data instances for the next mini-batch\n",
    "        batch_indices = indices[b*batch_size:(b+1)*batch_size]\n",
    "        batch_X = X_train[batch_indices]\n",
    "        batch_Y = Y_train[batch_indices]\n",
    "        \n",
    "        #  Run the network on each data instance in the minibatch\n",
    "        #  and then compute the object function value\n",
    "        pred_Y = net(batch_X)\n",
    "        loss = criterion(pred_Y, batch_Y)\n",
    "        \n",
    "        #  Back-propagate the gradient through the network using the\n",
    "        #  implicitly defined backward function, but zero out the\n",
    "        #  gradient first.\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #  Complete the mini-batch by actually updating the parameters.\n",
    "        for param in net.parameters():\n",
    "            param.data -= learning_rate * param.grad.data\n",
    "            \n",
    "    #  Print validation loss every 10 epochs\n",
    "    if ep != 0 and ep%10==0:\n",
    "        pred_Y_valid = net(X_valid)\n",
    "        valid_loss = criterion(pred_Y_valid, Y_valid)\n",
    "        print(\"Epoch %d loss: %.5f\" %(ep, valid_loss.data[0]))\n",
    "\n",
    "#  Compute and print the final training and test loss\n",
    "#  function values\n",
    "pred_Y_train = net(X_train)\n",
    "loss = criterion(pred_Y_train, Y_train)\n",
    "print('Final training loss is %.5f' %loss.data[0])\n",
    "\n",
    "pred_Y_test = net(X_test)\n",
    "test_loss = criterion(pred_Y_test, Y_test)\n",
    "print(\"Final test loss: %.5f\" %test_loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Success Rate\n",
    "\n",
    "The loss function value does not indicate the actual success rate.  In fact the MSE loss can in theory be arbitrarily bad and we could still have perfect classification.  By increasing the size of the training data and the number of epochs we can obtain better and better classification rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training success rate: 0.8203125\n",
      "Test success rate: 0.828125\n"
     ]
    }
   ],
   "source": [
    "def success_rate(pred_Y, Y):\n",
    "    '''\n",
    "    Calculate and return the success rate from the predicted output Y and the\n",
    "    expected output.  There are several issues to deal with.  First, the pred_Y\n",
    "    is non-binary, so the classification decision requires finding which column\n",
    "    index in each row of the prediction has the maximum value.  This is achieved\n",
    "    by using the torch.max() method, which returns both the maximum value and the\n",
    "    index of the maximum value; we want the latter.  We do this along the column,\n",
    "    which we indicate with the parameter 1.  Second, the once we have a 1-d vector\n",
    "    giving the index of the maximum for each of the predicted and target, we just\n",
    "    need to compare and count to get the number that are different.  We could do\n",
    "    using the Variable objects themselve, but it is easier syntactically to do this\n",
    "    using the .data Tensors for obscure PyTorch reasons.\n",
    "    '''\n",
    "    _,pred_Y_index = torch.max(pred_Y, 1)\n",
    "    _,Y_index = torch.max(Y,1)\n",
    "    num_equal = torch.sum(pred_Y_index.data == Y_index.data)\n",
    "    num_different = torch.sum(pred_Y_index.data != Y_index.data)\n",
    "    rate = num_equal / float(num_equal + num_different)\n",
    "    return rate\n",
    "\n",
    "print('Training success rate:', success_rate(pred_Y_train, Y_train))\n",
    "print('Test success rate:', success_rate(pred_Y_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cross Entropy, SoftMax and SGD\n",
    "\n",
    "As a final step in our initial learning of PyTorch, we'll examine use of the Cross-entropy loss function along with the softmax.  Then we'll examine use of built-in optimizers, in particular stochastic gradient descent (SGD).  Normally we think of SGD as having mini-batches and permutations associated with it, but that must be done explicitly here.  PyTorch does provide tools to help us, but this tutorial does not dig into them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, SGD requires the target outputs to be categorical indices as rather than binary vectors.  In addition, these must be tensors of longs rather than tensors of doubles.   We could go back and change the code at the very beginning to make both of these corrections, but instead we use properties of PyTorch to make the changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_categories(Y):\n",
    "    _, categories = torch.max(Y.data, 1)\n",
    "    categories = torch.Tensor.long(categories)\n",
    "    return Variable(categories)\n",
    "\n",
    "Y_test_c = convert_to_categories(Y_test)\n",
    "Y_train_c = convert_to_categories(Y_train)\n",
    "Y_valid_c = convert_to_categories(Y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation loss: 0.72988\n",
      "Epoch 10 loss: 0.71921\n",
      "Epoch 20 loss: 0.71302\n",
      "Epoch 30 loss: 0.70591\n",
      "Epoch 40 loss: 0.69471\n",
      "Epoch 50 loss: 0.68894\n",
      "Epoch 60 loss: 0.68712\n",
      "Epoch 70 loss: 0.68560\n",
      "Epoch 80 loss: 0.68430\n",
      "Epoch 90 loss: 0.68315\n",
      "Epoch 100 loss: 0.68209\n",
      "Epoch 110 loss: 0.68107\n",
      "Epoch 120 loss: 0.68013\n",
      "Epoch 130 loss: 0.67928\n",
      "Epoch 140 loss: 0.67853\n",
      "Epoch 150 loss: 0.67779\n",
      "Epoch 160 loss: 0.67709\n",
      "Epoch 170 loss: 0.67641\n",
      "Epoch 180 loss: 0.67579\n",
      "Epoch 190 loss: 0.67515\n",
      "Epoch 200 loss: 0.67447\n",
      "Epoch 210 loss: 0.67382\n",
      "Epoch 220 loss: 0.67316\n",
      "Epoch 230 loss: 0.67252\n",
      "Epoch 240 loss: 0.67181\n",
      "Epoch 250 loss: 0.67116\n",
      "Epoch 260 loss: 0.67050\n",
      "Epoch 270 loss: 0.66982\n",
      "Epoch 280 loss: 0.66904\n",
      "Epoch 290 loss: 0.66836\n",
      "Epoch 300 loss: 0.66761\n",
      "Epoch 310 loss: 0.66695\n",
      "Epoch 320 loss: 0.66622\n",
      "Epoch 330 loss: 0.66545\n",
      "Epoch 340 loss: 0.66467\n",
      "Epoch 350 loss: 0.66394\n",
      "Epoch 360 loss: 0.66323\n",
      "Epoch 370 loss: 0.66253\n",
      "Epoch 380 loss: 0.66165\n",
      "Epoch 390 loss: 0.66097\n",
      "Epoch 400 loss: 0.66008\n",
      "Epoch 410 loss: 0.65923\n",
      "Epoch 420 loss: 0.65857\n",
      "Epoch 430 loss: 0.65775\n",
      "Epoch 440 loss: 0.65693\n",
      "Epoch 450 loss: 0.65604\n",
      "Epoch 460 loss: 0.65518\n",
      "Epoch 470 loss: 0.65440\n",
      "Epoch 480 loss: 0.65368\n",
      "Epoch 490 loss: 0.65276\n",
      "Epoch 500 loss: 0.65186\n",
      "Epoch 510 loss: 0.65103\n",
      "Epoch 520 loss: 0.65020\n",
      "Epoch 530 loss: 0.64925\n",
      "Epoch 540 loss: 0.64827\n",
      "Epoch 550 loss: 0.64736\n",
      "Epoch 560 loss: 0.64646\n",
      "Epoch 570 loss: 0.64556\n",
      "Epoch 580 loss: 0.64438\n",
      "Epoch 590 loss: 0.64366\n",
      "Epoch 600 loss: 0.64255\n",
      "Epoch 610 loss: 0.64155\n",
      "Epoch 620 loss: 0.64063\n",
      "Epoch 630 loss: 0.63970\n",
      "Epoch 640 loss: 0.63852\n",
      "Epoch 650 loss: 0.63744\n",
      "Epoch 660 loss: 0.63641\n",
      "Epoch 670 loss: 0.63539\n",
      "Epoch 680 loss: 0.63429\n",
      "Epoch 690 loss: 0.63306\n",
      "Epoch 700 loss: 0.63210\n",
      "Epoch 710 loss: 0.63083\n",
      "Epoch 720 loss: 0.62960\n",
      "Epoch 730 loss: 0.62861\n",
      "Epoch 740 loss: 0.62760\n",
      "Epoch 750 loss: 0.62625\n",
      "Epoch 760 loss: 0.62509\n",
      "Epoch 770 loss: 0.62374\n",
      "Epoch 780 loss: 0.62250\n",
      "Epoch 790 loss: 0.62132\n",
      "Epoch 800 loss: 0.62021\n",
      "Epoch 810 loss: 0.61875\n",
      "Epoch 820 loss: 0.61744\n",
      "Epoch 830 loss: 0.61617\n",
      "Epoch 840 loss: 0.61477\n",
      "Epoch 850 loss: 0.61342\n",
      "Epoch 860 loss: 0.61188\n",
      "Epoch 870 loss: 0.61055\n",
      "Epoch 880 loss: 0.60921\n",
      "Epoch 890 loss: 0.60798\n",
      "Epoch 900 loss: 0.60647\n",
      "Epoch 910 loss: 0.60513\n",
      "Epoch 920 loss: 0.60404\n",
      "Epoch 930 loss: 0.60246\n",
      "Epoch 940 loss: 0.60105\n",
      "Epoch 950 loss: 0.59975\n",
      "Epoch 960 loss: 0.59812\n",
      "Epoch 970 loss: 0.59682\n",
      "Epoch 980 loss: 0.59543\n",
      "Epoch 990 loss: 0.59386\n",
      "Epoch 1000 loss: 0.59234\n",
      "Epoch 1010 loss: 0.59077\n",
      "Epoch 1020 loss: 0.58922\n",
      "Epoch 1030 loss: 0.58764\n",
      "Epoch 1040 loss: 0.58610\n",
      "Epoch 1050 loss: 0.58456\n",
      "Epoch 1060 loss: 0.58277\n",
      "Epoch 1070 loss: 0.58144\n",
      "Epoch 1080 loss: 0.58014\n",
      "Epoch 1090 loss: 0.57883\n",
      "Epoch 1100 loss: 0.57711\n",
      "Epoch 1110 loss: 0.57593\n",
      "Epoch 1120 loss: 0.57405\n",
      "Epoch 1130 loss: 0.57278\n",
      "Epoch 1140 loss: 0.57158\n",
      "Epoch 1150 loss: 0.56938\n",
      "Epoch 1160 loss: 0.56778\n",
      "Epoch 1170 loss: 0.56650\n",
      "Epoch 1180 loss: 0.56444\n",
      "Epoch 1190 loss: 0.56318\n",
      "Epoch 1200 loss: 0.56164\n",
      "Epoch 1210 loss: 0.55967\n",
      "Epoch 1220 loss: 0.55825\n",
      "Epoch 1230 loss: 0.55681\n",
      "Epoch 1240 loss: 0.55489\n",
      "Epoch 1250 loss: 0.55321\n",
      "Epoch 1260 loss: 0.55173\n",
      "Epoch 1270 loss: 0.55004\n",
      "Epoch 1280 loss: 0.54879\n",
      "Epoch 1290 loss: 0.54655\n",
      "Epoch 1300 loss: 0.54470\n",
      "Epoch 1310 loss: 0.54305\n",
      "Epoch 1320 loss: 0.54167\n",
      "Epoch 1330 loss: 0.53953\n",
      "Epoch 1340 loss: 0.53804\n",
      "Epoch 1350 loss: 0.53591\n",
      "Epoch 1360 loss: 0.53457\n",
      "Epoch 1370 loss: 0.53287\n",
      "Epoch 1380 loss: 0.53112\n",
      "Epoch 1390 loss: 0.52959\n",
      "Epoch 1400 loss: 0.52769\n",
      "Epoch 1410 loss: 0.52591\n",
      "Epoch 1420 loss: 0.52416\n",
      "Epoch 1430 loss: 0.52223\n",
      "Epoch 1440 loss: 0.52078\n",
      "Epoch 1450 loss: 0.51873\n",
      "Epoch 1460 loss: 0.51696\n",
      "Epoch 1470 loss: 0.51480\n",
      "Epoch 1480 loss: 0.51287\n",
      "Epoch 1490 loss: 0.51103\n",
      "Epoch 1500 loss: 0.50873\n",
      "Epoch 1510 loss: 0.50641\n",
      "Epoch 1520 loss: 0.50520\n",
      "Epoch 1530 loss: 0.50313\n",
      "Epoch 1540 loss: 0.50197\n",
      "Epoch 1550 loss: 0.50060\n",
      "Epoch 1560 loss: 0.49877\n",
      "Epoch 1570 loss: 0.49787\n",
      "Epoch 1580 loss: 0.49578\n",
      "Epoch 1590 loss: 0.49379\n",
      "Epoch 1600 loss: 0.49252\n",
      "Epoch 1610 loss: 0.49107\n",
      "Epoch 1620 loss: 0.49012\n",
      "Epoch 1630 loss: 0.48821\n",
      "Epoch 1640 loss: 0.48627\n",
      "Epoch 1650 loss: 0.48461\n",
      "Epoch 1660 loss: 0.48300\n",
      "Epoch 1670 loss: 0.48030\n",
      "Epoch 1680 loss: 0.47778\n",
      "Epoch 1690 loss: 0.47678\n",
      "Epoch 1700 loss: 0.47552\n",
      "Epoch 1710 loss: 0.47434\n",
      "Epoch 1720 loss: 0.47377\n",
      "Epoch 1730 loss: 0.47172\n",
      "Epoch 1740 loss: 0.47111\n",
      "Epoch 1750 loss: 0.46987\n",
      "Epoch 1760 loss: 0.46851\n",
      "Epoch 1770 loss: 0.46799\n",
      "Epoch 1780 loss: 0.46715\n",
      "Epoch 1790 loss: 0.46577\n",
      "Epoch 1800 loss: 0.46517\n",
      "Epoch 1810 loss: 0.46358\n",
      "Epoch 1820 loss: 0.46310\n",
      "Epoch 1830 loss: 0.46240\n",
      "Epoch 1840 loss: 0.46126\n",
      "Epoch 1850 loss: 0.45970\n",
      "Epoch 1860 loss: 0.45976\n",
      "Epoch 1870 loss: 0.45883\n",
      "Epoch 1880 loss: 0.45775\n",
      "Epoch 1890 loss: 0.45690\n",
      "Epoch 1900 loss: 0.45571\n",
      "Epoch 1910 loss: 0.45478\n",
      "Epoch 1920 loss: 0.45379\n",
      "Epoch 1930 loss: 0.45381\n",
      "Epoch 1940 loss: 0.45181\n",
      "Epoch 1950 loss: 0.45168\n",
      "Epoch 1960 loss: 0.45022\n",
      "Epoch 1970 loss: 0.45034\n",
      "Epoch 1980 loss: 0.44947\n",
      "Epoch 1990 loss: 0.44885\n",
      "Epoch 2000 loss: 0.44711\n",
      "Epoch 2010 loss: 0.44677\n",
      "Epoch 2020 loss: 0.44629\n",
      "Epoch 2030 loss: 0.44443\n",
      "Epoch 2040 loss: 0.44447\n",
      "Epoch 2050 loss: 0.44303\n",
      "Epoch 2060 loss: 0.44165\n",
      "Epoch 2070 loss: 0.44148\n",
      "Epoch 2080 loss: 0.43960\n",
      "Epoch 2090 loss: 0.43906\n",
      "Epoch 2100 loss: 0.43769\n",
      "Epoch 2110 loss: 0.43671\n",
      "Epoch 2120 loss: 0.43645\n",
      "Epoch 2130 loss: 0.43496\n",
      "Epoch 2140 loss: 0.43411\n",
      "Epoch 2150 loss: 0.43368\n",
      "Epoch 2160 loss: 0.43251\n",
      "Epoch 2170 loss: 0.43126\n",
      "Epoch 2180 loss: 0.42927\n",
      "Epoch 2190 loss: 0.42983\n",
      "Epoch 2200 loss: 0.42898\n",
      "Epoch 2210 loss: 0.42825\n",
      "Epoch 2220 loss: 0.42666\n",
      "Epoch 2230 loss: 0.42688\n",
      "Epoch 2240 loss: 0.42669\n",
      "Epoch 2250 loss: 0.42497\n",
      "Epoch 2260 loss: 0.42566\n",
      "Epoch 2270 loss: 0.42396\n",
      "Epoch 2280 loss: 0.42226\n",
      "Epoch 2290 loss: 0.42288\n",
      "Epoch 2300 loss: 0.42157\n",
      "Epoch 2310 loss: 0.42206\n",
      "Epoch 2320 loss: 0.41950\n",
      "Epoch 2330 loss: 0.41907\n",
      "Epoch 2340 loss: 0.41834\n",
      "Epoch 2350 loss: 0.41622\n",
      "Epoch 2360 loss: 0.41664\n",
      "Epoch 2370 loss: 0.41569\n",
      "Epoch 2380 loss: 0.41575\n",
      "Epoch 2390 loss: 0.41410\n",
      "Epoch 2400 loss: 0.41400\n",
      "Epoch 2410 loss: 0.41244\n",
      "Epoch 2420 loss: 0.41191\n",
      "Epoch 2430 loss: 0.41205\n",
      "Epoch 2440 loss: 0.41175\n",
      "Epoch 2450 loss: 0.41057\n",
      "Epoch 2460 loss: 0.41072\n",
      "Epoch 2470 loss: 0.40971\n",
      "Epoch 2480 loss: 0.40974\n",
      "Epoch 2490 loss: 0.40944\n",
      "Epoch 2500 loss: 0.40806\n",
      "Epoch 2510 loss: 0.40753\n",
      "Epoch 2520 loss: 0.40771\n",
      "Epoch 2530 loss: 0.40628\n",
      "Epoch 2540 loss: 0.40538\n",
      "Epoch 2550 loss: 0.40640\n",
      "Epoch 2560 loss: 0.40424\n",
      "Epoch 2570 loss: 0.40378\n",
      "Epoch 2580 loss: 0.40418\n",
      "Epoch 2590 loss: 0.40359\n",
      "Epoch 2600 loss: 0.40288\n",
      "Epoch 2610 loss: 0.40241\n",
      "Epoch 2620 loss: 0.40202\n",
      "Epoch 2630 loss: 0.40104\n",
      "Epoch 2640 loss: 0.40117\n",
      "Epoch 2650 loss: 0.40069\n",
      "Epoch 2660 loss: 0.40041\n",
      "Epoch 2670 loss: 0.39893\n",
      "Epoch 2680 loss: 0.39917\n",
      "Epoch 2690 loss: 0.39865\n",
      "Epoch 2700 loss: 0.39874\n",
      "Epoch 2710 loss: 0.39875\n",
      "Epoch 2720 loss: 0.39779\n",
      "Epoch 2730 loss: 0.39745\n",
      "Epoch 2740 loss: 0.39708\n",
      "Epoch 2750 loss: 0.39799\n",
      "Epoch 2760 loss: 0.39599\n",
      "Epoch 2770 loss: 0.39634\n",
      "Epoch 2780 loss: 0.39619\n",
      "Epoch 2790 loss: 0.39572\n",
      "Epoch 2800 loss: 0.39567\n",
      "Epoch 2810 loss: 0.39488\n",
      "Epoch 2820 loss: 0.39532\n",
      "Epoch 2830 loss: 0.39546\n",
      "Epoch 2840 loss: 0.39384\n",
      "Epoch 2850 loss: 0.39501\n",
      "Epoch 2860 loss: 0.39371\n",
      "Epoch 2870 loss: 0.39308\n",
      "Epoch 2880 loss: 0.39275\n",
      "Epoch 2890 loss: 0.39237\n",
      "Epoch 2900 loss: 0.39231\n",
      "Epoch 2910 loss: 0.39260\n",
      "Epoch 2920 loss: 0.39271\n",
      "Epoch 2930 loss: 0.39112\n",
      "Epoch 2940 loss: 0.39195\n",
      "Epoch 2950 loss: 0.38999\n",
      "Epoch 2960 loss: 0.39112\n",
      "Epoch 2970 loss: 0.39020\n",
      "Epoch 2980 loss: 0.38877\n",
      "Epoch 2990 loss: 0.38881\n",
      "Training success rate: 0.878125\n",
      "Test success rate: 0.8359375\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4)\n",
    "\n",
    "pred_Y_valid = net(X_valid)\n",
    "valid_loss = criterion(pred_Y_valid, Y_valid_c)\n",
    "print('Initial validation loss: %.5f' %valid_loss.data[0])\n",
    "\n",
    "num_epochs = 3000\n",
    "for ep in range(num_epochs):\n",
    "    #  Create a random permutation of the indices of the row vectors.\n",
    "    indices = torch.randperm(n_train)\n",
    "    \n",
    "    #  Run through each mini-batch\n",
    "    for b in range(n_batches):\n",
    "        #  Use slicing (of the pytorch Variable) to extract the\n",
    "        #  indices and then the data instances for the next mini-batch\n",
    "        batch_indices = indices[b*batch_size:(b+1)*batch_size]\n",
    "        batch_X = X_train[batch_indices]\n",
    "        batch_Y = Y_train_c[batch_indices]\n",
    "        \n",
    "        #  Run the network on each data instance in the minibatch\n",
    "        #  and then compute the object function value\n",
    "        pred_Y = net(batch_X)\n",
    "        loss = criterion(pred_Y, batch_Y)\n",
    "        \n",
    "        #  Back-propagate the gradient through the network using the\n",
    "        #  implicitly defined backward function, but zero out the\n",
    "        #  gradient first.  The step is made here by the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    pred_Y = net(X_train)\n",
    "    loss = criterion(pred_Y, Y_train_c)\n",
    "    if ep !=0 and ep%10==0:\n",
    "        pred_Y_valid = net(X_valid)\n",
    "        valid_loss = criterion(pred_Y_valid, Y_valid_c)\n",
    "        print(\"Epoch %d loss: %.5f\" %(ep, valid_loss.data[0]))\n",
    "\n",
    "pred_Y_train = net(X_train)\n",
    "pred_Y_test = net(X_test)\n",
    "print('Training success rate:', success_rate(pred_Y_train, Y_train))\n",
    "print('Test success rate:', success_rate(pred_Y_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "These results are by no means perfect and can be easily improved.  However, our emphasis has been on the basics.  I strongly recommend studying these examples and playing with similar code on your own.  Then, just jump into the HW 6 assignment.  You really can write the solutions with relatively little code.  You are also welcome to use as much of the code I've provided here as you wish.  The only thing I have not covered that you need is the implementation of convolutional networks, but you can find examples that will help you get started there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
