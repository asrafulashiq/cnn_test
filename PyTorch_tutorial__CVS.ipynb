{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to PyTorch and Neural Networks\n",
    "#### Charles Stewart\n",
    "#### November 26, 2017\n",
    "\n",
    "This is a brief tutorial introduction to PyTorch using a specific example of building\n",
    "a neural network that learns to classify points as inside or outside a hyper sphere\n",
    "of a given radius.\n",
    "\n",
    "We'll start with the problem itself, working purely in NumPy in order to clarify\n",
    "what we are trying to do.  After this, we'll introduce the basic notations of PyTorch, and proceed to two different neural network solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated points in array of dimension (400, 2)\n",
      "Binary labels are in a second array of dimension (400, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYNQcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1711329aa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "def gen_from_sphere( n, center, r_min, r_max):\n",
    "    '''\n",
    "    Generate n points in random directions from a center that are at least\n",
    "    r_min away and at most r_max away.  If r_min is 0 then the points are\n",
    "    within a hyper sphere of radius r_max.  Otherwise, the points are within a\n",
    "    spherical shell whose minimum radius is r_min and whose maximum radius\n",
    "    is r_max.  Note that the points generated in this simple way are not\n",
    "    uniformly distributed in Euclidean space, but that is not a major concern\n",
    "    here.\n",
    "    \n",
    "    The dimensionality of the space is inferred from the dimension of the center\n",
    "    point.  \n",
    "    \n",
    "    Return an array where each row is a data point.  I.e. the array in n X dim\n",
    "    '''\n",
    "    dim = center.size\n",
    "    points = np.broadcast_to(center.reshape(1,dim),(n,dim)) \n",
    "    directions = 2*np.random.random((n,dim))-1\n",
    "    directions /= np.linalg.norm(directions,axis=1).reshape((n,1))\n",
    "    offsets = np.random.random(n) * (r_max-r_min) + r_min\n",
    "    points = points + directions * offsets.reshape((n,1))\n",
    "    return points\n",
    "\n",
    "\n",
    "def gen_points_nested_spheres( n, center, r_min, r_max, debug=False):\n",
    "    '''\n",
    "    Generate n data points in hypersphere, half of which - the \"inliers\" --\n",
    "    are within r_min of the center and half of which -- the \"outliers\" -- are\n",
    "    between r_min and r_max.  Returned are the points in an n X d array\n",
    "    (dim is the dimension of the center point) and a n X 2 binary array\n",
    "    labeling whether the points are inliers (1,0) or outliers (0,1).\n",
    "    \n",
    "    If the debug flag is on, the array shape of the points and the labels\n",
    "    are output.  In addition, if the dimension is 2 plot is generated.\n",
    "    '''\n",
    "    num_inliers = n//2\n",
    "    num_outliers = n - num_inliers\n",
    "    inlier_X = gen_from_sphere( num_inliers, center, 0, r_min)\n",
    "    inlier_Y = np.broadcast_to( np.array([1.,0.]), (num_inliers,2))\n",
    "    outlier_X = gen_from_sphere( num_outliers, center, r_min, r_max)\n",
    "    outlier_Y = np.broadcast_to( np.array([0.,1.]), (num_outliers,2))\n",
    "    X = np.concatenate( (inlier_X,outlier_X), axis=0)\n",
    "    Y = np.concatenate( (inlier_Y,outlier_Y), axis=0)\n",
    "    \n",
    "    if debug:\n",
    "        print('Generated points in array of dimension', X.shape)\n",
    "        print('Binary labels are in a second array of dimension', Y.shape)\n",
    "        \n",
    "        if center.size == 2:\n",
    "            from matplotlib import pyplot as plt\n",
    "            plt.scatter( inlier_X[:,0], inlier_X[:,1], marker='+' )\n",
    "            plt.scatter( outlier_X[:,0], outlier_X[:,1], marker='o' )\n",
    "            plt.show()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# Generate an example in 2d, just to see the point distribution\n",
    "# The \"real\" data \n",
    "num_points = 400\n",
    "center = np.array( [1.5, 6.4])\n",
    "r_min, r_max = 5, 10\n",
    "X,Y = gen_points_nested_spheres( num_points, center, r_min, r_max, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "Now for PyTorch.  The starting point is http://pytorch.org/tutorials/ for tutorials and http://pytorch.org/docs/master/ for extensive documentation.  On the tutorials site, I found both the \"60 Minute Blitz\" and \"Learning PyTorch with Examples\" to be useful, but preferred the latter.  It starts with a simple neural network implemented in NumPy, then converts this to use of pytorch Tensors.  Next it introduces Variables and their most important property, automatic gradient calculation (\"autograd\").  At that point it has two subsections, one on defining new autograd functions and the second comparing to TensorFlow Static Graphs.  Neither of these is terribly important on first reading.  Instead, I'd suggest skipping ahead to the nn module.  I suggest going back to the 60 Minute Blitz for example of convolutional layes\n",
    "\n",
    "We'll start here with a brief discussion of Tensors and Variables and then jump right into Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tensor: w\n",
      "\n",
      " 0.0652 -1.2703 -0.6925\n",
      " 1.1215  0.8265 -1.2306\n",
      " 1.0633  0.2786 -2.7459\n",
      " 1.2412 -0.7543 -0.5722\n",
      " 0.8647  1.2853  0.5834\n",
      "-0.1743  2.1687 -0.1302\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# The first important property of torch is the notion of a Tensor.  This is essentially\n",
    "# the same as a NumPy array, but it has CUDA / GPU capabilities when a GPU is available.\n",
    "# We can easily convert back and forth between Tensors and arrays without deep copying\n",
    "\n",
    "# The following creates a 2d tensor with 6 rows and 3 columns, filled in with random\n",
    "# values from a normal distribution of mean 0 and variance 1.\n",
    "w = torch.randn(6, 3)\n",
    "print('First tensor: w')\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced verson of W is x:\n",
      "\n",
      " 1.0633  0.2786\n",
      " 1.2412 -0.7543\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "We changed x[0,1] to 20.0 and now w[2,1] is also 20.0\n"
     ]
    }
   ],
   "source": [
    "# Tensors can be sliced and this works without deep copying\n",
    "x = w[2:4, 0:2]\n",
    "print('Sliced verson of W is x:')\n",
    "print(x)\n",
    "x[0,1] = 20\n",
    "print('We changed x[0,1] to', x[0,1], 'and now w[2,1] is also', w[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-component vector as a tensor: \n",
      " 0.0987\n",
      " 1.2697\n",
      "-0.6193\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Component-wise multiplication, with broadcasting\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size at d:\\downloads\\pytorch-master-1\\torch\\lib\\th\\generic/THTensorMath.c:847",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3dff2c761940>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'3-component vector as a tensor:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Component-wise multiplication, with broadcasting'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m     \u001b[0m__rmul__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: inconsistent tensor size at d:\\downloads\\pytorch-master-1\\torch\\lib\\th\\generic/THTensorMath.c:847"
     ]
    }
   ],
   "source": [
    "#  Create a row vector and then do component-wise multiplication\n",
    "u = torch.randn( 3 )\n",
    "print('3-component vector as a tensor:', u)\n",
    "print('Component-wise multiplication, with broadcasting')\n",
    "print(w*u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.8768\n",
      " 2.4567\n",
      " 2.2077\n",
      " 2.0801\n",
      " 0.3124\n",
      "-0.2643\n",
      "[torch.FloatTensor of size 6x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Now create a 3x1 tensor--- essentially a column vector --- and do\n",
    "#  matrix multiplication\n",
    "u = torch.randn( 3,1 )\n",
    "print( torch.mm(w,u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is the numpy version of u:\n",
      " [[ 1.1636337 ]\n",
      " [-0.08808575]\n",
      " [-0.99503493]]\n",
      "b and c are back to pytorch: \n",
      " 1.1636\n",
      "-0.0881\n",
      "-0.9950\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "We changed a[0,0] and this changes b and c as well:\n",
      "b[0,0] = 5.0 and c[0,0] = 5.0\n"
     ]
    }
   ],
   "source": [
    "#  Many of the methods you'd expect for NumPy arrays exist for PyTorch tensors.  See\n",
    "#  extensive documentation at http://pytorch.org/docs/master/torch.html#tensors\n",
    "\n",
    "# We can go back and forth between NumPy arrays and PyTorch tensors\n",
    "# through shallow copies\n",
    "a = u.numpy()\n",
    "print('a is the numpy version of u:\\n', a)\n",
    "b = torch.from_numpy(a)\n",
    "c = torch.Tensor(b)\n",
    "print('b and c are back to pytorch:', b)\n",
    "a[0,0] = 5\n",
    "print('We changed a[0,0] and this changes b and c as well:')\n",
    "print('b[0,0] =', b[0,0], 'and c[0,0] =', c[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor c is \n",
      " 1.1269\n",
      " 6.7593\n",
      " 6.2868\n",
      " 6.8417\n",
      " 3.6296\n",
      "-0.9330\n",
      "[torch.cuda.FloatTensor of size 6x1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So why create a separate class for tensors when they seem to have\n",
    "# the same attributes as as NumPy arrays?  There are two answers.\n",
    "\n",
    "# First, tensors are designed to work with CUDA when a GPU is available,\n",
    "# but this is transparent to the rest of the code:\n",
    "if torch.cuda.is_available():\n",
    "    w = w.cuda()\n",
    "    b = b.cuda()\n",
    "\n",
    "c = torch.mm(w,b)\n",
    "print('Tensor c is', c)  # Proceeds whether on CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Variables\n",
    "The second reason for PyTorch tensors is that they are designed to work with our second\n",
    "major PyTorch construct:  Variables.  A Variable has three major attributes:\n",
    "1. data --- A PyTorch tensor.   \n",
    "2. grad --- The gradient tensor of the variable with respect to the final cost function it is used in\n",
    "3. creator - The link to the function the Variable is created from.  More on this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminding ourselves:  w= \n",
      "  0.0652  -1.2703  -0.6925\n",
      "  1.1215   0.8265  -1.2306\n",
      "  1.0633  20.0000  -2.7459\n",
      "  1.2412  -0.7543  -0.5722\n",
      "  0.8647   1.2853   0.5834\n",
      " -0.1743   2.1687  -0.1302\n",
      "[torch.cuda.FloatTensor of size 6x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Reminding ourselves:  w=', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv= Variable containing:\n",
      "  0.0652  -1.2703  -0.6925\n",
      "  1.1215   0.8265  -1.2306\n",
      "  1.0633  20.0000  -2.7459\n",
      "  1.2412  -0.7543  -0.5722\n",
      "  0.8647   1.2853   0.5834\n",
      " -0.1743   2.1687  -0.1302\n",
      "[torch.cuda.FloatTensor of size 6x3 (GPU 0)]\n",
      "\n",
      "wv.data = \n",
      "  0.0652  -1.2703  -0.6925\n",
      "  1.1215   0.8265  -1.2306\n",
      "  1.0633  20.0000  -2.7459\n",
      "  1.2412  -0.7543  -0.5722\n",
      "  0.8647   1.2853   0.5834\n",
      " -0.1743   2.1687  -0.1302\n",
      "[torch.cuda.FloatTensor of size 6x3 (GPU 0)]\n",
      "\n",
      "Slicing works on variables: Variable containing:\n",
      " 20.0000  -2.7459\n",
      " -0.7543  -0.5722\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now create a variable from w, telling the constructor that we want\n",
    "# to be able to compute gradients on this variable.\n",
    "wv = Variable(w, requires_grad=True)\n",
    "print(\"wv=\", wv)\n",
    "print('wv.data =', wv.data)\n",
    "print('Slicing works on variables:', wv[2:4,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is Variable containing:\n",
      " 23.5639\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Next create a simple cost function\n",
    "cost = torch.mean( wv*wv )  # Cost is the square magnitude of the elments of the matrix\n",
    "print('cost is', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now comes the gradient!\n",
    "By calling the method *backward*, we backpropagate the gradient through the computation used to create the cost function value.  In this simple case, we are computing the gradient with respect to the terms of our our small $m \\times n$ ($ 6 \\times 3$) 2d tensor.  The result should be another 2d tensor whose values are $\\frac{2}{m n} w[i,j]$. When we ask for the values of the gradient we get exactly what we predicted.  For example, the 2,2 entry in the gradient, corresponding to the value 20 in wv, is $2/18\\times 20$ as the following shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv.grad= Variable containing:\n",
      " 0.0072 -0.1411 -0.0769\n",
      " 0.1246  0.0918 -0.1367\n",
      " 0.1181  2.2222 -0.3051\n",
      " 0.1379 -0.0838 -0.0636\n",
      " 0.0961  0.1428  0.0648\n",
      "-0.0194  0.2410 -0.0145\n",
      "[torch.cuda.FloatTensor of size 6x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost.backward()\n",
    "print('wv.grad=', wv.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creator / grad_fn\n",
    "\n",
    "In order to briefly discuss the creator function, we take this further in the following example by combining several variables, similar to the computation in a fully connected layer of a neural network.  This combination can be thought of as a simple graph where the root is the cost function, each node in the middle is the result of an intermediate function, and our leaves are the initial variables we are interested in. The nodes have grad_fn that point back to the previous nodes (or leaves) combined to create them.  Leaves have no grad_fn.\n",
    "\n",
    "The print statements from the following only show these grad_fn values, but if you wish you can add print statements to calculate the gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.addmm received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor), but expected one of:\n * (torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, !torch.FloatTensor!, out=torch.cuda.FloatTensor)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, !torch.cuda.FloatTensor!, !torch.FloatTensor!, out=torch.cuda.FloatTensor)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-295524db9185>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mav\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mmm\u001b[1;34m(self, matrix)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_static_blas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36m_static_blas\u001b[1;34m(cls, args, inplace)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_args\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_blas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\_functions\\blas.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[1;32m---> 24\u001b[1;33m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: torch.addmm received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor), but expected one of:\n * (torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, !torch.FloatTensor!, out=torch.cuda.FloatTensor)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, !torch.cuda.FloatTensor!, !torch.FloatTensor!, out=torch.cuda.FloatTensor)\n"
     ]
    }
   ],
   "source": [
    "av = Variable(torch.randn( 3, 1),requires_grad = True)\n",
    "bv = Variable(torch.randn( 6, 1),requires_grad = True )\n",
    "mult = torch.mm(wv,av) + bv\n",
    "cost = torch.mean(mult)\n",
    "cost.backward()\n",
    "print('cost.grad_fn:', cost.grad_fn)\n",
    "print('mult.grad_fn:', mult.grad_fn)\n",
    "print('av.grad_fn:', av.grad_fn)\n",
    "print('bv.grad_fn:', bv.grad_fn)\n",
    "print('wv.grad_fn:', wv.grad_fn)\n",
    "print('av.grad:', av.grad)\n",
    "print('mult.grad:', mult.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks in PyTorch\n",
    "Using Variables and the build up of functions between them, we can create neural network by hand.  Doing so is a good exercise in understanding the basics of PyTorch and if you are going to do significant work in neural networks I recommend that you do so.  For our purposes, however, we are going to switch gears to the higher-level capabilities in PyTorch for defining an using neural networks from standard components.\n",
    "\n",
    "Before getting started, we return to our hypersphere classification problem, and generate the data we will actually use to train, validata and test our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated points in array of dimension (640, 4)\n",
      "Binary labels are in a second array of dimension (640, 2)\n",
      "generate_and_convert for Train  X size  torch.Size([640, 4]) , Y size torch.Size([640, 2])\n",
      "Generated points in array of dimension (128, 4)\n",
      "Binary labels are in a second array of dimension (128, 2)\n",
      "generate_and_convert for Valid  X size  torch.Size([128, 4]) , Y size torch.Size([128, 2])\n",
      "Generated points in array of dimension (128, 4)\n",
      "Binary labels are in a second array of dimension (128, 2)\n",
      "generate_and_convert for Valid  X size  torch.Size([128, 4]) , Y size torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "#  The points, the center location and the dimensions of the \n",
    "#  internal hypersphere and the external disk.\n",
    "point_dim = 4\n",
    "center = 10*np.random.randn(point_dim)\n",
    "r_min, r_max = 4, 8\n",
    "\n",
    "def generate_and_convert( n, center, r_min, r_max, debug=True, name=''):\n",
    "    '''\n",
    "    Generate the data for the interior and exterior points and convert\n",
    "    to PyTorch variables.\n",
    "    '''\n",
    "    X, Y = gen_points_nested_spheres( n, center, r_min, r_max, debug)\n",
    "    X, Y = Variable(torch.Tensor(X)), Variable(torch.Tensor(Y))\n",
    "    if debug:\n",
    "        print('generate_and_convert for', name, ' X size ', X.size(), ', Y size', Y.size())\n",
    "    return X, Y\n",
    "\n",
    "n_train, n_valid, n_test = 640, 128, 128\n",
    "X_train, Y_train = generate_and_convert( n_train, center, r_min, r_max, debug=True, name='Train')\n",
    "X_valid, Y_valid = generate_and_convert( n_valid, center, r_min, r_max, debug=True, name='Valid')\n",
    "X_test, Y_test = generate_and_convert( n_test, center, r_min, r_max, debug=True, name='Valid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _type of \n",
       "    1     0\n",
       "    1     0\n",
       "    1     0\n",
       "     ⋮      \n",
       "    0     1\n",
       "    0     1\n",
       "    0     1\n",
       "[torch.FloatTensor of size 640x2]\n",
       ">"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.data.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a simple fully connected network\n",
    "\n",
    "The cell below shows the definition of a neural network with two hidden layers of 15 nodes each.\n",
    "It is defined as a subclass of the nn.Module class, so the first step in the initializer is to call the initializer for the base class.\n",
    "The initializer for our new class creates the layers --- each fully-connected --- but does not hook them together.  This is done in the forward function.\n",
    "You will also notice that there is no definition of the backward function, even though it is used.\n",
    "This function is created in the base class based on the result of the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (fc1): Linear (4 -> 15)\n",
      "  (fc2): Linear (15 -> 15)\n",
      "  (fc3): Linear (15 -> 2)\n",
      ")\n",
      "torch.Size([15, 4])\n",
      "torch.Size([15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15])\n",
      "torch.Size([2, 15])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "N0 = point_dim\n",
    "N1 = 15\n",
    "N2 = 15\n",
    "Nout = 2  \n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Create three fully connected layers, two of which are hidden and the third is\n",
    "        # the output layer.  In each case, the first argument o Linear is the number\n",
    "        # of input values from the previous layer, and the second argument is the number\n",
    "        # of nodes in this layer.  The call to the Linear initializer creates a PyTorch\n",
    "        # functional that in turn adds a weight matrix and a bias vector to the list of\n",
    "        # (learnable) parameters stored with each Net object.  These weight matrices\n",
    "        # and bias vectors are implicitly initialized using a normal distribution\n",
    "        # with mean 0 and variance 1\n",
    "        self.fc1 = nn.Linear( N0, N1, bias=True)\n",
    "        self.fc2 = nn.Linear( N1, N2, bias=True)\n",
    "        self.fc3 = nn.Linear( N2, Nout, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  The forward method takes an input Variable and creates a chain of Variables\n",
    "        #  from the layers of the network defined in the initializer. The F.relu is\n",
    "        #  a functional implementing the Rectified Linear activation function.\n",
    "        #  Notice that the output layer does not include the activation function.\n",
    "        #  As we will see, that is combined into the criterion for the loss function.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "#  Create an instance of this network.\n",
    "net = Net()\n",
    "\n",
    "#  Define the Mean Squared error loss function as the criterion for this network's training\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#  Print a summary of the network.  Notice that this only shows the layers\n",
    "print(net)\n",
    "\n",
    "#  Print a s\n",
    "params = list(net.parameters())\n",
    "print(params[0].size()) # The parameter holding the layer 1 weight matrix\n",
    "print(params[1].size()) # ... the layer 1 bias vector\n",
    "print(params[2].size()) # ... the layer 2 weight matrix\n",
    "print(params[3].size()) # ... the layer 3 bias vector\n",
    "print(params[4].size()) # ... the layer 4 weight vector\n",
    "print(params[5].size()) # ... the layer 5 bias vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " -5.7651  18.1887  -9.5608 -21.9431\n",
       " -5.7810  15.7852 -10.9445 -19.6453\n",
       " -3.7042  14.4837  -9.0501 -21.8035\n",
       " -7.2994  15.5639 -10.0577 -21.2829\n",
       " -4.7000  15.4076  -9.6713 -20.8807\n",
       " -3.2930  15.7744  -9.0998 -21.5830\n",
       " -4.3674  16.4139  -7.3873 -18.5105\n",
       " -4.7603  15.9239  -8.9830 -21.5980\n",
       " -4.5969  15.0407  -9.1389 -21.3012\n",
       " -4.6759  15.3777  -9.6598 -20.8851\n",
       " -4.6292  15.3638 -10.0143 -20.8075\n",
       " -2.8834  17.5589  -9.9961 -18.2863\n",
       " -5.5793  16.9445 -10.7775 -19.1637\n",
       " -4.3517  15.5882  -9.3459 -20.9285\n",
       " -3.6153  15.8405 -11.4896 -19.9670\n",
       " -4.6795  15.3855  -9.7135 -20.8990\n",
       " -3.3478  16.8425  -8.8735 -21.8627\n",
       " -4.8689  16.3649 -10.1900 -21.7220\n",
       " -2.7208  14.8643  -8.6006 -21.5294\n",
       " -4.2736  15.6821  -9.1319 -21.7064\n",
       " -4.1486  15.7536 -10.4890 -19.8859\n",
       " -4.5887  16.0944  -9.4415 -20.4993\n",
       " -5.8996  15.1089  -8.1425 -20.9493\n",
       " -4.4570  15.3020  -7.1289 -19.5296\n",
       " -5.5113  15.2851 -11.5070 -22.8082\n",
       " -4.2402  15.9420 -10.5197 -19.7649\n",
       " -5.9853  14.1812 -12.2009 -19.2742\n",
       " -4.4406  14.6646  -8.7497 -20.7022\n",
       " -5.2730  15.8944  -8.7064 -20.4678\n",
       " -4.7358  15.4335  -9.7758 -21.1560\n",
       " -3.1657  16.9575 -10.2427 -20.0148\n",
       " -4.1708  15.5302 -12.2314 -20.6814\n",
       " -3.8721  14.8627  -9.8567 -20.8157\n",
       " -6.5304  17.2188  -9.4168 -21.3494\n",
       " -4.0360  16.7208 -11.0955 -22.7349\n",
       " -4.7663  15.5624  -9.6397 -21.0286\n",
       " -5.3972  12.9316  -7.6466 -21.0950\n",
       " -3.6966  16.5048 -11.5210 -20.3375\n",
       " -3.1049  17.0432 -10.7059 -19.7043\n",
       " -4.9988  14.0965 -10.4467 -20.8718\n",
       " -6.8564  13.4271 -11.8326 -19.3392\n",
       " -4.4823  15.0322  -8.6506 -21.2435\n",
       " -3.7371  12.9528  -7.2006 -19.8185\n",
       " -5.0342  15.9402  -8.9781 -20.6404\n",
       " -3.4296  14.6492  -9.8980 -21.0958\n",
       " -2.3603  13.1275 -10.6602 -19.8063\n",
       " -3.3149  15.8249  -8.5340 -21.9125\n",
       " -6.9619  15.0792 -11.5241 -21.2836\n",
       " -4.7674  16.7640  -9.5507 -19.3175\n",
       " -5.4171  14.6863  -9.4068 -21.3134\n",
       " -4.8191  15.4456  -9.6074 -21.0396\n",
       " -5.6821  14.7104 -10.6189 -20.7907\n",
       " -6.0743  14.1280  -7.8732 -22.7571\n",
       " -5.4563  16.0847  -9.5192 -19.7851\n",
       " -5.2351  14.9086  -9.2869 -21.1592\n",
       " -2.3145  14.4437  -7.4658 -22.4718\n",
       " -5.5224  16.6483  -9.7296 -20.3424\n",
       " -4.8333  17.4730  -9.7955 -21.1123\n",
       " -4.9689  17.9894  -9.6110 -19.9069\n",
       " -4.3013  14.9830  -7.8994 -19.6791\n",
       " -5.5583  15.2719  -9.7663 -20.6631\n",
       " -5.3859  16.7787  -8.1139 -21.1851\n",
       " -5.5305  14.3719  -8.7893 -22.1561\n",
       " -3.1973  15.9618 -12.1670 -20.8061\n",
       " -1.0888  18.7935 -11.4520 -22.6144\n",
       " -3.2611  12.8520  -7.3777 -22.5444\n",
       " -4.1633  13.8463 -16.0342 -17.6574\n",
       " -0.4881  10.1143  -9.6455 -23.1786\n",
       " -0.5418  12.2569 -12.1820 -17.3523\n",
       " -7.8582  17.7498 -10.1982 -22.0366\n",
       " -7.7346  16.2094 -11.1648 -23.1712\n",
       " -5.9409  11.9294 -16.4804 -21.3399\n",
       " -0.4099  10.9367 -10.5386 -20.5422\n",
       " -3.2577  18.7962 -15.3905 -21.0208\n",
       " -6.6330  13.2464 -12.5981 -19.5586\n",
       " -0.6025   9.5868  -8.8880 -17.6992\n",
       " -4.1856  18.8366 -11.0489 -17.5032\n",
       " -0.3682  17.1666  -9.3376 -20.7198\n",
       " -0.4085  19.4062  -8.7490 -16.6926\n",
       "  0.7082  11.8128 -12.2047 -21.0535\n",
       " -8.4821  17.4412  -4.6392 -22.0741\n",
       " -1.5941  12.8162 -11.7316 -21.0529\n",
       " -2.7037  12.0653  -6.7043 -23.5540\n",
       " -4.4049  18.8767  -6.6403 -21.5184\n",
       " -0.5101  19.8182  -9.8053 -24.4988\n",
       " -8.4201  18.9138 -10.7726 -21.5175\n",
       " -1.5778  17.6924 -12.0937 -23.7405\n",
       " -4.8695  16.5226 -15.8169 -18.1389\n",
       " -1.5993  18.9321  -7.3461 -22.8749\n",
       " -2.3921  13.6069  -7.4362 -18.6747\n",
       "  1.4591  15.6428 -13.1961 -21.4381\n",
       " -0.7152   9.4661  -9.7427 -21.9915\n",
       " -9.6858  15.2639  -6.1519 -20.2024\n",
       " -2.2595  12.1163 -12.1576 -24.0032\n",
       " -4.8823  19.0529  -4.8470 -17.2083\n",
       " -9.7623  18.9510 -13.3491 -18.5633\n",
       "  0.0194  15.4388 -15.1344 -23.0811\n",
       " -6.7277  13.0321 -10.6161 -17.7108\n",
       " -0.3620  13.2194  -6.3276 -20.1990\n",
       " -5.5165  11.1270 -15.2933 -21.8156\n",
       " -0.1981  10.4823  -6.6443 -23.7558\n",
       " -1.2678  11.3066  -8.3464 -24.2679\n",
       " -2.5205  17.6556  -5.7915 -16.5254\n",
       " -4.3131   8.6958  -8.3814 -18.3821\n",
       " -7.3474  11.1011 -11.2865 -16.7228\n",
       " -4.4456  18.9025 -16.1795 -20.0522\n",
       " -1.1898  21.5565 -11.9552 -20.5500\n",
       " -5.5532  18.7175 -14.0427 -17.6515\n",
       " -8.5995   9.9041 -10.5675 -17.7702\n",
       " -2.5636  11.0736  -8.1641 -26.2293\n",
       " -1.0577  12.3208 -13.3069 -22.7985\n",
       " -7.1866  18.9662  -9.8320 -23.2508\n",
       " -0.9469   9.9442  -9.7577 -16.8307\n",
       " -3.1738  17.7779  -6.0363 -23.2919\n",
       " -1.8500  19.3954  -3.9131 -18.4915\n",
       " -5.2510  12.3022  -4.9626 -17.2125\n",
       " -4.9188  11.4846 -10.6603 -20.1377\n",
       " -3.8887  17.3194 -12.3877 -25.4503\n",
       " -6.5076  13.3002  -6.5893 -23.6440\n",
       " -7.0162  18.6916 -13.8724 -21.5666\n",
       " -5.7603  19.8949  -7.9994 -20.9916\n",
       " -3.0622  13.9269 -13.9982 -22.4359\n",
       " -7.7028  13.5274  -9.6463 -24.3845\n",
       " -6.5898  10.3600  -9.2471 -22.7079\n",
       "  0.0138  16.5671 -10.1712 -18.7783\n",
       " -6.7871  12.2134 -14.6133 -24.7067\n",
       " -1.7655  10.7392  -7.5399 -19.1069\n",
       " -7.7394  11.7553  -9.0786 -24.7771\n",
       "[torch.FloatTensor of size 128x4]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Training the network\n",
    "\n",
    "Now we can write our own training function using a form of stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 1.55322\n",
      "Epoch 10 loss: 0.55543\n",
      "Epoch 20 loss: 0.35502\n",
      "Epoch 30 loss: 0.28331\n",
      "Epoch 40 loss: 0.26098\n",
      "Epoch 50 loss: 0.24959\n",
      "Epoch 60 loss: 0.23999\n",
      "Epoch 70 loss: 0.23130\n",
      "Epoch 80 loss: 0.22404\n",
      "Epoch 90 loss: 0.21795\n",
      "Epoch 100 loss: 0.21261\n",
      "Epoch 110 loss: 0.20753\n",
      "Epoch 120 loss: 0.20252\n",
      "Epoch 130 loss: 0.19755\n",
      "Epoch 140 loss: 0.19293\n",
      "Epoch 150 loss: 0.18864\n",
      "Epoch 160 loss: 0.18474\n",
      "Epoch 170 loss: 0.18106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-dda3d2ef1780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m#  gradient first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m#  Complete the mini-batch by actually updating the parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradient has to be a Tensor, Variable or None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  Set parameters to control the process\n",
    "epochs = 1000\n",
    "batch_size = 16\n",
    "n_batches = int(np.ceil(n_train / batch_size))\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#  Compute an initial loss using all of the validation data.\n",
    "#\n",
    "#  A couple of notes are important here:\n",
    "#  (1) X_valid contains all of the validation input, with each validation\n",
    "#      data instance being a row of X_valid\n",
    "#  (2) Therefore, pred_Y_valid is a Variable containing the output layer\n",
    "#      activations for each of the validation inputs.\n",
    "#  (3) This is accomplished through the function call net(X_valid), which in\n",
    "#      turn calls the forward method under the hood to figure out the flow of\n",
    "#      the data and activations in the network.\n",
    "pred_Y_valid = net(X_valid)\n",
    "valid_loss = criterion(pred_Y_valid, Y_valid)\n",
    "print(\"Initial loss: %.5f\" %valid_loss.data[0])\n",
    "\n",
    "for ep in range(epochs):\n",
    "    #  Create a random permutation of the indices of the row vectors.\n",
    "    indices = torch.randperm(n_train)\n",
    "    \n",
    "    #  Run through each mini-batch\n",
    "    for b in range(n_batches):\n",
    "        #  Use slicing (of the pytorch Variable) to extract the\n",
    "        #  indices and then the data instances for the next mini-batch\n",
    "        batch_indices = indices[b*batch_size:(b+1)*batch_size]\n",
    "        batch_X = X_train[batch_indices]\n",
    "        batch_Y = Y_train[batch_indices]\n",
    "        \n",
    "        #  Run the network on each data instance in the minibatch\n",
    "        #  and then compute the object function value\n",
    "        pred_Y = net(batch_X)\n",
    "        loss = criterion(pred_Y, batch_Y)\n",
    "        \n",
    "        #  Back-propagate the gradient through the network using the\n",
    "        #  implicitly defined backward function, but zero out the\n",
    "        #  gradient first.\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #  Complete the mini-batch by actually updating the parameters.\n",
    "        for param in net.parameters():\n",
    "            param.data -= learning_rate * param.grad.data\n",
    "            \n",
    "    #  Print validation loss every 10 epochs\n",
    "    if ep != 0 and ep%10==0:\n",
    "        pred_Y_valid = net(X_valid)\n",
    "        valid_loss = criterion(pred_Y_valid, Y_valid)\n",
    "        print(\"Epoch %d loss: %.5f\" %(ep, valid_loss.data[0]))\n",
    "\n",
    "#  Compute and print the final training and test loss\n",
    "#  function values\n",
    "pred_Y_train = net(X_train)\n",
    "loss = criterion(pred_Y_train, Y_train)\n",
    "print('Final training loss is %.5f' %loss.data[0])\n",
    "\n",
    "pred_Y_test = net(X_test)\n",
    "test_loss = criterion(pred_Y_test, Y_test)\n",
    "print(\"Final test loss: %.5f\" %test_loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Success Rate\n",
    "\n",
    "The loss function value does not indicate the actual success rate.  In fact the MSE loss can in theory be arbitrarily bad and we could still have perfect classification.  By increasing the size of the training data and the number of epochs we can obtain better and better classification rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate(pred_Y, Y):\n",
    "    '''\n",
    "    Calculate and return the success rate from the predicted output Y and the\n",
    "    expected output.  There are several issues to deal with.  First, the pred_Y\n",
    "    is non-binary, so the classification decision requires finding which column\n",
    "    index in each row of the prediction has the maximum value.  This is achieved\n",
    "    by using the torch.max() method, which returns both the maximum value and the\n",
    "    index of the maximum value; we want the latter.  We do this along the column,\n",
    "    which we indicate with the parameter 1.  Second, the once we have a 1-d vector\n",
    "    giving the index of the maximum for each of the predicted and target, we just\n",
    "    need to compare and count to get the number that are different.  We could do\n",
    "    using the Variable objects themselve, but it is easier syntactically to do this\n",
    "    using the .data Tensors for obscure PyTorch reasons.\n",
    "    '''\n",
    "    _,pred_Y_index = torch.max(pred_Y, 1)\n",
    "    _,Y_index = torch.max(Y,1)\n",
    "    num_equal = torch.sum(pred_Y_index.data == Y_index.data)\n",
    "    num_different = torch.sum(pred_Y_index.data != Y_index.data)\n",
    "    rate = num_equal / float(num_equal + num_different)\n",
    "    return rate\n",
    "\n",
    "print('Training success rate:', success_rate(pred_Y_train, Y_train))\n",
    "print('Test success rate:', success_rate(pred_Y_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cross Entropy, SoftMax and SGD\n",
    "\n",
    "As a final step in our initial learning of PyTorch, we'll examine use of the Cross-entropy loss function along with the softmax.  Then we'll examine use of built-in optimizers, in particular stochastic gradient descent (SGD).  Normally we think of SGD as having mini-batches and permutations associated with it, but that must be done explicitly here.  PyTorch does provide tools to help us, but this tutorial does not dig into them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, SGD requires the target outputs to be categorical indices as rather than binary vectors.  In addition, these must be tensors of longs rather than tensors of doubles.   We could go back and change the code at the very beginning to make both of these corrections, but instead we use properties of PyTorch to make the changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_categories(Y):\n",
    "    _, categories = torch.max(Y.data, 1)\n",
    "    categories = torch.Tensor.long(categories)\n",
    "    return Variable(categories)\n",
    "\n",
    "Y_test_c = convert_to_categories(Y_test)\n",
    "Y_train_c = convert_to_categories(Y_train)\n",
    "Y_valid_c = convert_to_categories(Y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4)\n",
    "\n",
    "pred_Y_valid = net(X_valid)\n",
    "valid_loss = criterion(pred_Y_valid, Y_valid_c)\n",
    "print('Initial validation loss: %.5f' %valid_loss.data[0])\n",
    "\n",
    "num_epochs = 3000\n",
    "for ep in range(num_epochs):\n",
    "    #  Create a random permutation of the indices of the row vectors.\n",
    "    indices = torch.randperm(n_train)\n",
    "    \n",
    "    #  Run through each mini-batch\n",
    "    for b in range(n_batches):\n",
    "        #  Use slicing (of the pytorch Variable) to extract the\n",
    "        #  indices and then the data instances for the next mini-batch\n",
    "        batch_indices = indices[b*batch_size:(b+1)*batch_size]\n",
    "        batch_X = X_train[batch_indices]\n",
    "        batch_Y = Y_train_c[batch_indices]\n",
    "        \n",
    "        #  Run the network on each data instance in the minibatch\n",
    "        #  and then compute the object function value\n",
    "        pred_Y = net(batch_X)\n",
    "        loss = criterion(pred_Y, batch_Y)\n",
    "        \n",
    "        #  Back-propagate the gradient through the network using the\n",
    "        #  implicitly defined backward function, but zero out the\n",
    "        #  gradient first.  The step is made here by the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    pred_Y = net(X_train)\n",
    "    loss = criterion(pred_Y, Y_train_c)\n",
    "    if ep !=0 and ep%10==0:\n",
    "        pred_Y_valid = net(X_valid)\n",
    "        valid_loss = criterion(pred_Y_valid, Y_valid_c)\n",
    "        print(\"Epoch %d loss: %.5f\" %(ep, valid_loss.data[0]))\n",
    "\n",
    "pred_Y_train = net(X_train)\n",
    "pred_Y_test = net(X_test)\n",
    "print('Training success rate:', success_rate(pred_Y_train, Y_train))\n",
    "print('Test success rate:', success_rate(pred_Y_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "These results are by no means perfect and can be easily improved.  However, our emphasis has been on the basics.  I strongly recommend studying these examples and playing with similar code on your own.  Then, just jump into the HW 6 assignment.  You really can write the solutions with relatively little code.  You are also welcome to use as much of the code I've provided here as you wish.  The only thing I have not covered that you need is the implementation of convolutional networks, but you can find examples that will help you get started there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
